"""
Streamlitãƒ™ãƒ¼ã‚¹ã®Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé¢¨ã®UIã§æ¡ä»¶ã‚’å…¥åŠ›ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
"""

import streamlit as st
import sys
from pathlib import Path
import os
import pandas as pd

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent))

from mercari.scraper import MercariScraper
from common.utils import save_to_csv
import time
import re


def extract_price(price_str: str) -> float:
    """ä¾¡æ ¼æ–‡å­—åˆ—ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º"""
    if not price_str:
        return float('inf')
    price_match = re.search(r'[\d,]+', price_str.replace(',', ''))
    if price_match:
        try:
            return float(price_match.group().replace(',', ''))
        except:
            return float('inf')
    return float('inf')


def run_scraping(search_keyword: str, max_items: int, compare_with_amazon: bool):
    """
    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    
    Args:
        search_keyword: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        max_items: å–å¾—ä»¶æ•°
        compare_with_amazon: Amazonã¨æ¯”è¼ƒã™ã‚‹ã‹
    """
    # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
    if os.path.exists(os.path.expanduser('~/playwright-browsers')):
        os.environ['PLAYWRIGHT_BROWSERS_PATH'] = os.path.expanduser('~/playwright-browsers')
    
    items_data = []
    
    try:
        with MercariScraper(headless=True) as scraper:  # Streamlitã§ã¯headless=Trueæ¨å¥¨
            # å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰å•†å“ãƒªãƒ³ã‚¯ã‚’å–å¾—
            target_url = f"https://www.mercari.com/jp/search/?keyword={search_keyword}"
            
            st.info(f"å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {target_url}")
            item_links = scraper.scrape_list(target_url)
            
            if not item_links:
                st.error("å•†å“ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return None
            
            st.success(f"{len(item_links)} ä»¶ã®å•†å“ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ä½œæˆ
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # å•†å“æƒ…å ±ã‚’å–å¾—
            successful_count = 0
            for i, item_url in enumerate(item_links[:max_items * 2]):  # ä½™è£•ã‚’æŒã£ã¦å–å¾—
                if successful_count >= max_items:
                    break
                
                status_text.text(f"å•†å“ {successful_count + 1}/{max_items} ã‚’å–å¾—ä¸­... ({i+1}/{len(item_links)})")
                progress_bar.progress((i + 1) / min(len(item_links), max_items * 2))
                
                try:
                    item_info = scraper.scrape_detail(item_url)
                    if item_info:
                        title = item_info.get('title', '')
                        if title and len(title) > 5:
                            items_data.append(item_info)
                            successful_count += 1
                except Exception as e:
                    st.warning(f"ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
                
                time.sleep(1)  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
            
            progress_bar.progress(1.0)
            status_text.text("å®Œäº†ï¼")
        
        return items_data
    
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        st.code(traceback.format_exc())
        return None


# Streamlitã‚¢ãƒ—ãƒªã®è¨­å®š
st.set_page_config(
    page_title="ãƒ¡ãƒ«ã‚«ãƒªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ›’",
    layout="wide"
)

st.title("ğŸ›’ ãƒ¡ãƒ«ã‚«ãƒªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("---")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¨­å®šãƒ•ã‚©ãƒ¼ãƒ 
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    search_keyword = st.text_input(
        "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
        value="ãƒã‚±ãƒ¢ãƒ³ã‚«ãƒ¼ãƒ‰",
        help="ãƒ¡ãƒ«ã‚«ãƒªã§æ¤œç´¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
    )
    
    max_items = st.number_input(
        "å–å¾—ä»¶æ•°",
        min_value=1,
        max_value=20,
        value=5,
        help="å–å¾—ã™ã‚‹å•†å“ã®æœ€å¤§ä»¶æ•°"
    )
    
    compare_with_amazon = st.checkbox(
        "Amazonã¨ä¾¡æ ¼æ¯”è¼ƒ",
        value=False,
        help="Amazonã®ä¾¡æ ¼ã¨æ¯”è¼ƒã™ã‚‹å ´åˆã¯ãƒã‚§ãƒƒã‚¯ï¼ˆé–²è¦§ãƒ¢ãƒ¼ãƒ‰ï¼‰"
    )
    
    st.markdown("---")
    st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ:\n- æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦å®Ÿè¡Œã§ãã¾ã™\n- å–å¾—ä»¶æ•°ã‚’èª¿æ•´ã§ãã¾ã™\n- Amazonæ¯”è¼ƒã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
col1, col2 = st.columns([3, 1])

with col1:
    st.header("ğŸ“‹ å®Ÿè¡Œæ¡ä»¶")
    
    # æ¡ä»¶ã‚’è¡¨ç¤º
    st.write(f"**æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:** {search_keyword}")
    st.write(f"**å–å¾—ä»¶æ•°:** {max_items}ä»¶")
    st.write(f"**Amazonæ¯”è¼ƒ:** {'æœ‰åŠ¹' if compare_with_amazon else 'ç„¡åŠ¹'}")

with col2:
    st.header("ğŸš€ å®Ÿè¡Œ")
    
    if st.button("â–¶ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ", type="primary", use_container_width=True):
        # å®Ÿè¡Œä¸­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        with st.spinner("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­..."):
            items_data = run_scraping(search_keyword, max_items, compare_with_amazon)
        
        if items_data:
            st.success(f"âœ… {len(items_data)}ä»¶ã®å•†å“æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸï¼")
            
            # çµæœã‚’è¡¨ç¤º
            st.header("ğŸ“Š å–å¾—çµæœ")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
            df = pd.DataFrame(items_data)
            
            # è¡¨ç¤ºç”¨ã«ã‚«ãƒ©ãƒ ã‚’é¸æŠ
            display_columns = ['title', 'price', 'url']
            if 'amazon_price' in df.columns:
                display_columns.extend(['amazon_price', 'price_difference'])
            
            if display_columns:
                st.dataframe(df[display_columns], use_container_width=True)
            
            # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            csv_data = df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv_data,
                file_name=f"mercari_items_{search_keyword}_{int(time.time())}.csv",
                mime="text/csv"
            )
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            output_path = Path(__file__).parent / "mercari" / "output" / f"mercari_items_{search_keyword}_{int(time.time())}.csv"
            save_to_csv(items_data, str(output_path))
            st.info(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: `{output_path}`")
        else:
            st.error("å•†å“æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

# å±¥æ­´ã‚»ã‚¯ã‚·ãƒ§ãƒ³
st.markdown("---")
st.header("ğŸ“ éå»ã®å®Ÿè¡Œçµæœ")

# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€è¦§è¡¨ç¤º
output_dir = Path(__file__).parent / "mercari" / "output"
if output_dir.exists():
    csv_files = list(output_dir.glob("*.csv"))
    if csv_files:
        csv_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        for csv_file in csv_files[:5]:  # æœ€æ–°5ä»¶
            with st.expander(f"ğŸ“„ {csv_file.name}"):
                try:
                    df = pd.read_csv(csv_file)
                    st.dataframe(df, use_container_width=True)
                    
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                    with open(csv_file, 'rb') as f:
                        st.download_button(
                            label=f"ğŸ“¥ {csv_file.name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=f.read(),
                            file_name=csv_file.name,
                            mime="text/csv",
                            key=f"download_{csv_file.name}"
                        )
                except Exception as e:
                    st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
